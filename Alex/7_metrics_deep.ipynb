{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1SdULf_GZ8sdFYNWI7RAywpZjRAKvb0uQ","timestamp":1698863044962},{"file_id":"1KVlO9v06i2aOeez9t6UdsZ3_JjvcUZhb","timestamp":1695739931599},{"file_id":"1OyraH-ey6XXoO9hS6GxxdB41FckjzIds","timestamp":1695725359577},{"file_id":"11LX9E2LKLKI2rcwXqYGph9lj5gl_EghT","timestamp":1695638174006},{"file_id":"1JupsVmgPJIsVt5rbMTTtjDch4SsJeC-G","timestamp":1694164828229}],"authorship_tag":"ABX9TyNV8VMvD4CCv1SI6wPxqpS5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pathlib\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn import svm\n","from sklearn import model_selection\n","from statsmodels.tools.eval_measures import mse\n","from sklearn.metrics import mean_absolute_error\n","import keras.layers\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","from google.colab import files\n","import re\n","from sklearn import preprocessing\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.model_selection import KFold\n","pd.options.display.float_format = '{:.5f}'.format\n","from sklearn.utils import shuffle\n","\n"],"metadata":{"id":"mpo2pkNIHR8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasetbig = pd.read_csv(\"/content/sample_data/base_dataset.csv\")\n","# If you are using the extra dataset please set this to true\n","bol_extra_dataset=False"],"metadata":{"id":"5a0MErvZFKrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PrintDot(keras.callbacks.Callback):\n","  def on_epoch_end(self,epoch,logs):\n","    if epoch % 100 == 0: print(\"\")\n","    print(\".\",end=\"\")\n","\n","def popAndGetPredictionLabels(bol_extra_dataset,train_dataset):\n","\n","  if bol_extra_dataset == False:\n","    train_labels=[\n","      [\"success_rate\",train_dataset.pop(\"success_rate\")],\n","      [\"collision_rate\", train_dataset.pop(\"collision_rate\")],\n","      [\"timeout_rate\", train_dataset.pop(\"timeout_rate\")],\n","      [\"average_path_length\", train_dataset.pop(\"average_path_length\")],\n","      [\"average_time_diff\", train_dataset.pop(\"average_time_diff\")]\n","    ]\n","  else:\n","    train_labels=[\n","      [\"success_rate\",train_dataset.pop(\"success_rate\")],\n","      [\"collision_rate\", train_dataset.pop(\"collision_rate\")],\n","      [\"average_collision_amount\", train_dataset.pop(\"average_collision_amount\")],\n","      [\"timeout_rate\", train_dataset.pop(\"timeout_rate\")],\n","      [\"timeout_collision_rate\",train_dataset.pop(\"timeout_collision_rate\")],\n","      [\"average_path_length\", train_dataset.pop(\"average_path_length\")],\n","      [\"average_time_diff\", train_dataset.pop(\"average_time_diff\")]\n","\n","    ]\n","  return train_labels\n","\n","def get_group(dataset, performanceMetric):\n","  return dataset.loc[dataset[\"Label\"]== performanceMetric]\n","\n","def get_best_of(dataset,performanceMetric,measure):\n","  dataset = get_group(dataset,performanceMetric)\n","  result = dataset[dataset[measure] == dataset[measure].min()]\n","  return result\n","\n","def norm(dataset):\n","  train_stats = dataset.describe()\n","  train_stats = train_stats.transpose()\n","  return ((dataset-train_stats[\"min\"])/(train_stats[\"max\"]-train_stats[\"min\"]))\n","\n","def is_unique(s):\n","    a = s.to_numpy()\n","    return (a[0] == a).all()\n","\n","def checkForConstants(dataset):\n","  for column in dataset:\n","      if is_unique(dataset[column]) == True:\n","          print(\"Dropping\", column)\n","          dataset=dataset.drop(columns=column)\n","  return dataset\n","\n","def getMeans(bol_extra_dataset, dataset):\n","  if(bol_extra_dataset == False):\n","    means = [\n","      [\"mean_success_rate\",             dataset[\"success_rate\"].mean()],\n","      [\"mean_collision_rate\",           dataset[\"collision_rate\"].mean()],\n","      [\"mean_timeout_rate\",             dataset[\"timeout_rate\"].mean()],\n","      [\"mean_average_path_length\",      dataset[\"average_path_length\"].mean()],\n","      [\"mean_average_time_diff\",        dataset[\"average_time_diff\"].mean()]\n","      ]\n","  else:\n","    means = [\n","      [\"mean_success_rate\",             dataset[\"success_rate\"].mean()],\n","      [\"mean_collision_rate\",           dataset[\"collision_rate\"].mean()],\n","      [\"mean_average_collision_amount\", dataset[\"average_collision_amount\"].mean()],\n","      [\"mean_timeout_rate\",             dataset[\"timeout_rate\"].mean()],\n","      [\"mean_timeout_collision_rate\",   dataset[\"timeout_collision_rate\"].mean()],\n","      [\"mean_average_path_length\",      dataset[\"average_path_length\"].mean()],\n","      [\"mean_average_time_diff\",        dataset[\"average_time_diff\"].mean()]\n","      ]\n","  return means\n","\n","def get_numpy_labels(bol_extra_dataset,dataset):\n","  if(bol_extra_dataset==False):\n","    success_rate        =np.hstack([dataset[:,2:3]])\n","    collision_rate      =np.hstack([dataset[:,3:4]])\n","    timeout_rate        =np.hstack([dataset[:,4:5]])\n","    average_path_length =np.hstack([dataset[:,5:6]])\n","    average_time_diff   =np.hstack([dataset[:,6:7]])\n","    label_array=[success_rate,collision_rate,timeout_rate,average_path_length,average_time_diff]\n","  else:\n","    success_rate            =np.hstack([dataset[:,2:3]])\n","    collision_rate          =np.hstack([dataset[:,3:4]])\n","    average_collision_amount=np.hstack([dataset[:,4:5]])\n","    timeout_rate            =np.hstack([dataset[:,5:6]])\n","    timeout_collision_rate  =np.hstack([dataset[:,6:7]])\n","    average_path_length     =np.hstack([dataset[:,7:8]])\n","    average_time_diff       =np.hstack([dataset[:,8:9]])\n","    label_array=[success_rate,collision_rate,average_collision_amount,\n","                 timeout_rate,timeout_collision_rate,average_path_length,average_time_diff]\n","  return label_array"],"metadata":{"id":"qWpoLhgb7ni5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasetbig=shuffle(datasetbig,random_state=0)"],"metadata":{"id":"1jaikoBXu7Sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)"],"metadata":{"id":"iEIdvj1_tZFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rlca = datasetbig[datasetbig[\"rlca\"] == 1]\n","crowdnav = datasetbig[datasetbig[\"crowdnav\"] == 1]\n","dwa = datasetbig[datasetbig[\"dwa\"] == 1]\n","\n","indoor = datasetbig[datasetbig[\"indoor_map_type\"] == 1]\n","outdoor = datasetbig[datasetbig[\"outdoor_map_type\"] == 1]"],"metadata":{"id":"BTJpsnNQebff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# choose dataset to work with\n","dataset = datasetbig"],"metadata":{"id":"erUZrloSFOxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.drop(columns=[\"teb\"])"],"metadata":{"id":"TxiRaTqAFYj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["means = getMeans(bol_extra_dataset,dataset)"],"metadata":{"id":"1InrQKMB92qJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = checkForConstants(dataset)"],"metadata":{"id":"WnU9mkb9uhmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nump=dataset[\"robot_max_speed\"].to_numpy()"],"metadata":{"id":"_6rtPC9uumgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = dataset.sample(frac=0.8, random_state = 0)\n","test_dataset = dataset.drop(train_dataset.index)\n","\n","temp = train_dataset.sample(frac=0.8, random_state = 0)\n","predict_dataset = train_dataset.drop(temp.index)\n","train_dataset = train_dataset.drop(predict_dataset.index)"],"metadata":{"id":"VBaoDMSTvIXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_array = np.array(train_dataset)\n","test_dataset_array = np.array(test_dataset)\n","predict_dataset_array = np.array(predict_dataset)\n","\n","output_train_array=get_numpy_labels(bol_extra_dataset,train_dataset_array)\n","output_test_array=get_numpy_labels(bol_extra_dataset,test_dataset_array)\n","output_predict_array=get_numpy_labels(bol_extra_dataset,predict_dataset_array)"],"metadata":{"id":"bZBL68unMG5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels = popAndGetPredictionLabels(bol_extra_dataset,train_dataset)\n","test_labels = popAndGetPredictionLabels(bol_extra_dataset,test_dataset)\n","predict_labels = popAndGetPredictionLabels(bol_extra_dataset,predict_dataset)"],"metadata":{"id":"g4zEA48bxB0a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Scaling data via standard scaler"],"metadata":{"id":"mpJK3e3hr1Uy"}},{"cell_type":"code","source":["normed_train_data_std = preprocessing.StandardScaler().fit_transform(train_dataset)\n","normed_test_data_std = preprocessing.StandardScaler().fit_transform(test_dataset)\n","normed_predict_data_std = preprocessing.StandardScaler().fit_transform(predict_dataset)"],"metadata":{"id":"v94gWshMyIWw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Scaling via formular"],"metadata":{"id":"PZiaTUmiMgRZ"}},{"cell_type":"code","source":["normed_train_data_form=norm(train_dataset)\n","normed_test_data_form=norm(test_dataset)\n","normed_predict_data_form=norm(predict_dataset)"],"metadata":{"id":"WKOW-FlqyMU_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DNN"],"metadata":{"id":"uAp7S8YNBgjH"}},{"cell_type":"code","source":["formular = True\n","standard_scaler = False\n","\n","if formular == True and standard_scaler == False:\n","  normed_train_data = normed_train_data_form\n","  normed_test_data = normed_test_data_form\n","  normed_predict_data = normed_predict_data_form\n","\n","if formular == False and standard_scaler == True:\n","  normed_train_data = normed_train_data_std\n","  normed_test_data = normed_test_data_std\n","  normed_predict_data = normed_predict_data_std\n"],"metadata":{"id":"VJsnZ50ht_RL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model():\n","    model = keras.Sequential([\n","        layers.Dense(units=24, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n","        layers.Dense(units=16, activation=tf.nn.relu),\n","        layers.Dense(units=1)\n","    ])\n","\n","    optimizer = tf.keras.optimizers.RMSprop(0.001)\n","\n","    model.compile(\n","        loss=\"mae\",\n","        optimizer=optimizer,\n","        metrics=[\"mae\"]\n","             )\n","    return model\n"],"metadata":{"id":"YNaU8gL5yOKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import layers\n","model = build_model()"],"metadata":{"id":"MTKgNe2RxyRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["performanceOverview = pd.DataFrame()\n","performanceOverview"],"metadata":{"id":"Cwae_Ud59w5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["patienceNumbers = [20,30,40,80]\n","\n","for patienceNumber in patienceNumbers:\n","  for label_idx in range(len(train_labels)):\n","    print(train_labels[label_idx][0])\n","    from tensorflow.keras import layers\n","    model = build_model()\n","    EPOCHS = 1000\n","\n","    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patienceNumber)\n","\n","    history = model.fit(\n","        normed_train_data,\n","        train_labels[label_idx][1],\n","        epochs=EPOCHS,\n","        validation_split = 0.2,\n","        verbose=0,\n","        batch_size = 64,\n","        callbacks=[early_stop, PrintDot()])\n","\n","\n","    hist = pd.DataFrame(history.history)\n","    hist[\"epoch\"] = history.epoch\n","    hist.tail()\n","\n","    histogram_data = pd.DataFrame(data=hist.tail(1))\n","    histogram_data=histogram_data.reset_index()\n","    histogram_data.astype('int64')\n","    histogram_data.round(4)\n","\n","    layers = len(model.layers)\n","    params = model.count_params()\n","\n","    evaluation_loss =\"no loss\"\n","    evaluation_mae =\"no mae\"\n","    evaluation_mse =\"no mse\"\n","\n","    evaluation_loss,evaluation_mse = model.evaluate(normed_test_data, test_labels[label_idx][1],verbose = 0)\n","    prediction = model.predict(normed_predict_data)\n","\n","    mean_series = pd.Series(means[label_idx][1])\n","    s = pd.Series(means[label_idx][1])\n","    for idx in range(len(predict_labels[label_idx][1])-1):\n","      mean_series = pd.concat([mean_series,s])\n","\n","    prediction_R2  = r2_score(output_predict_array[label_idx],prediction)\n","    prediction_MSE = mse(output_predict_array[label_idx],prediction)\n","    prediction_MAE = mean_absolute_error(output_predict_array[label_idx],prediction)\n","    MAE_mean = mean_absolute_error(mean_series,prediction)\n","\n","    n = len(normed_train_data)\n","    x = len(normed_train_data.columns)\n","    Adjusted_R2=1-((1-prediction_R2)*(n-1))/(n-x-1)\n","    SSE = np.sum((output_predict_array[label_idx] - prediction) ** 2)\n","    RMSE=prediction_MSE.mean()**0.5\n","    MAPE = mean_absolute_percentage_error(output_predict_array[label_idx],prediction)\n","\n","    model_data = pd.DataFrame()\n","    model_data = {\n","          \"Datapoints\":[len(train_dataset.index)],\n","          \"Label\":[train_labels[label_idx][0]],\n","          \"Layers\":[layers],\n","          \"Params\":[params],\n","          \"Patience\":[patienceNumber],\n","          \"base_mean\":[means[label_idx][1]],\n","          \"evaluation_Loss\":[evaluation_loss],\n","          \"evaluation_mae\":[evaluation_mae],\n","          \"evaluation_mse\":[evaluation_mse],\n","          \"Diff_base_MAE\": [means[label_idx][1]-prediction_MAE],\n","          \"prediction_R2\"   :[prediction_R2],\n","          \"prediction_adj.R2\":[Adjusted_R2],\n","          \"prediction_MAE\"  :[prediction_MAE],\n","          \"prediciton_MSE\":[prediction_MSE.mean()],\n","          \"SSE\":[SSE],\n","          \"RMSE\":[RMSE],\n","          \"MAPE\":[MAPE],\n","          \"MAE_mean\":[MAE_mean]\n","      }\n","\n","    model_data_df = pd.DataFrame(data=model_data)\n","\n","    result = pd.concat([model_data_df, histogram_data],axis=1)\n","    performanceOverview = pd.concat([performanceOverview,result])\n","\n","dt = datetime.now()\n","performanceOverview.to_csv(\"/content/sample_data/performanceOverview \"+dt.isoformat()+\".csv\")\n"],"metadata":{"id":"HL4KsG6SAAlN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result=get_best_of(performanceOverview,\"average_time_diff\",\"prediction_MAE\")\n","result[[\"base_mean\",\"prediction_R2\",\"prediction_adj.R2\",\"prediction_MAE\",\"prediciton_MSE\"]]"],"metadata":{"id":"ihzPymoqByB3"},"execution_count":null,"outputs":[]}]}